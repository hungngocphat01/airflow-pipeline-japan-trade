# (WIP) Apache Airflow pipeline and Spark/Hadoop practice

Projected final pipeline

![Expected pipeline](assets/draft_pipeline.jpeg)

## Data sources
- [Japanese trade statistics from 1988 to 2019](https://www.kaggle.com/datasets/zanjibar/100-million-data-csv) (~100M rows)
- [HS Code (2017)](https://github.com/datasets/harmonized-system/)
- [Japanese customs country code](https://www.customs.go.jp/toukei/sankou/code/country_e.htm)

## Technologies used 
- Apache Airflow (`docker-compose` taken from [data-engineering-zoomcamp](https://github.com/DataTalksClub/data-engineering-zoomcamp))
- Apache Spark
- Apache Hadoop 
- Google Cloud Services

## How to